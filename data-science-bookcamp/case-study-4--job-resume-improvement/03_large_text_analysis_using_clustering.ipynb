{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-large-text-analysis-using-clustering.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOhtMMzTDs5JMOvPXaxLkEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-science-research-and-practice/blob/main/data-science-bookcamp/case-study-4--job-resume-improvement/03_large_text_analysis_using_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Large text analysis using Clustering"
      ],
      "metadata": {
        "id": "CKQ3xygyzGoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we execute NLP on large collections\n",
        "of real-world texts. This type of analysis is seemingly straightforward, given the\n",
        "techniques presented thus far. For example, suppose we’re doing market research\n",
        "across multiple online discussion forums. Each forum is composed of hundreds of\n",
        "users who discuss a specific topic, such as politics, fashion, technology, or cars. We\n",
        "want to automatically extract all the discussion topics based on the contents of the\n",
        "user conversations. These extracted topics will be used to plan a marketing campaign,\n",
        "which will target users based on their online interests.\n",
        "\n",
        "How do we cluster user discussions into topics? \n",
        "\n",
        "One approach would be to do the following:\n",
        "1. Convert all discussion texts into a matrix of word counts\n",
        "2. Dimensionally reduce the word count matrix using singular value decomposition (SVD). This will allow us to efficiently complete all pairs of text similarities with matrix multiplication.\n",
        "3. Utilize the matrix of text similarities to cluster the discussions into topics.\n",
        "4. Explore the topic clusters to identify useful topics for our marketing campaign.\n",
        "\n"
      ],
      "metadata": {
        "id": "fJsYonsIz1lJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "nEaM8U6Lz_Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "gtDg8Oyyz__L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sin, cos\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TobbrwVl0CWF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##20Newsgroup dataset"
      ],
      "metadata": {
        "id": "tRYlC_k-0OTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usenet, which is a well-established online collection\n",
        "of discussion forums, are called newsgroups. Each individual\n",
        "newsgroup focuses on some topic of discussion, which is briefly outlined in the newsgroup name.\n",
        "\n",
        "We can load these newsgroup posts by importing `fetch_20newsgroups` from `sklearn.datasets`."
      ],
      "metadata": {
        "id": "sTg51MOn0QLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the newsgroup dataset\n",
        "newsgroups = fetch_20newsgroups(remove=(\"headers\", \"footers\"))"
      ],
      "metadata": {
        "id": "WCepvD5r0Wq4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The newsgroups object contains posts from 20 different newsgroups."
      ],
      "metadata": {
        "id": "uvofbc7T05_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the names of all 20 newsgroups\n",
        "print(newsgroups.target_names)"
      ],
      "metadata": {
        "id": "9gsaUHqR06bp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14bc4d2b-a070-40e0-b29b-7a65315024ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(newsgroups.target_names))"
      ],
      "metadata": {
        "id": "SHYUPbuJ1gpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230ad590-49b7-4ceb-c40a-dbb4044d77e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s turn our attention to the actual newsgroup texts, which are stored as a list in the newsgroups.data attribute."
      ],
      "metadata": {
        "id": "w2n8UnF4fk1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the first newsgroup post\n",
        "print(newsgroups.data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANhKc1npf3HO",
        "outputId": "ffa8cdae-4635-4d1a-9378-c9721bb69d80"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the newsgroup name at index 0\n",
        "origin = newsgroups.target_names[newsgroups.target[0]]\n",
        "print(f\"The post at index 0 first appeared in the '{origin}' group.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcYizeuvtt-g",
        "outputId": "060d8752-bdd7-4137-fb64-bb583b7f41a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The post at index 0 first appeared in the 'rec.autos' group.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s dive deeper into our newsgroup dataset by printing out the dataset size."
      ],
      "metadata": {
        "id": "aJ3Hq8b4uFwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of newsgroup posts\n",
        "dataset_size = len(newsgroups.data)\n",
        "print(f\"Our dataset contains {dataset_size} newsgroup posts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42LzwACquGMm",
        "outputId": "230b796c-9880-48f7-9a23-2d291aa7e732"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our dataset contains 11314 newsgroup posts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vectorizing documents"
      ],
      "metadata": {
        "id": "KoPVq13vhrpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need\n",
        "to efficiently compute newsgroup post similarities by representing our text data as a\n",
        "matrix. \n",
        "\n",
        "To do so, we need to transform each newsgroup post into a term-frequency\n",
        "(TF) vector.\n",
        "\n",
        "Scikit-learn provides a `CountVectorizer` class for transforming input texts into TF vectors."
      ],
      "metadata": {
        "id": "i3yrzb_ohs4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing a TF matrix\n",
        "vectorizer = CountVectorizer()\n",
        "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
        "print(tf_matrix)"
      ],
      "metadata": {
        "id": "fcDHOYXThzU9",
        "outputId": "fe62a2b7-944b-4403-c769-474d2c53ccca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 108644)\t4\n",
            "  (0, 110106)\t1\n",
            "  (0, 57577)\t2\n",
            "  (0, 24398)\t2\n",
            "  (0, 79534)\t1\n",
            "  (0, 100942)\t1\n",
            "  (0, 37154)\t1\n",
            "  (0, 45141)\t1\n",
            "  (0, 70570)\t1\n",
            "  (0, 78701)\t2\n",
            "  (0, 101084)\t4\n",
            "  (0, 32499)\t4\n",
            "  (0, 92157)\t1\n",
            "  (0, 100827)\t6\n",
            "  (0, 79461)\t1\n",
            "  (0, 39275)\t1\n",
            "  (0, 60326)\t2\n",
            "  (0, 42332)\t1\n",
            "  (0, 96432)\t1\n",
            "  (0, 67137)\t1\n",
            "  (0, 101732)\t1\n",
            "  (0, 27703)\t1\n",
            "  (0, 49871)\t2\n",
            "  (0, 65338)\t1\n",
            "  (0, 14106)\t1\n",
            "  :\t:\n",
            "  (11313, 55901)\t1\n",
            "  (11313, 93448)\t1\n",
            "  (11313, 97535)\t1\n",
            "  (11313, 93393)\t1\n",
            "  (11313, 109366)\t1\n",
            "  (11313, 102215)\t1\n",
            "  (11313, 29148)\t1\n",
            "  (11313, 26901)\t1\n",
            "  (11313, 94401)\t1\n",
            "  (11313, 89686)\t1\n",
            "  (11313, 80827)\t1\n",
            "  (11313, 72219)\t1\n",
            "  (11313, 32984)\t1\n",
            "  (11313, 82912)\t1\n",
            "  (11313, 99934)\t1\n",
            "  (11313, 96505)\t1\n",
            "  (11313, 72102)\t1\n",
            "  (11313, 32981)\t1\n",
            "  (11313, 82692)\t1\n",
            "  (11313, 101854)\t1\n",
            "  (11313, 66399)\t1\n",
            "  (11313, 63405)\t1\n",
            "  (11313, 61366)\t1\n",
            "  (11313, 7462)\t1\n",
            "  (11313, 109600)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the data type\n",
        "print(type(tf_matrix))"
      ],
      "metadata": {
        "id": "KlMzEhydvUtn",
        "outputId": "c1603662-0f0d-4a1a-fabc-8632505210d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting a CSR matrix to a NumPy array\n",
        "tf_np_matrix = tf_matrix.toarray()\n",
        "print(tf_np_matrix)"
      ],
      "metadata": {
        "id": "OQ3Az3Wpvci9",
        "outputId": "a511e55b-8339-4b27-b6a9-0cd62d86b5f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the vocabulary size\n",
        "assert tf_np_matrix.shape == tf_matrix.shape\n",
        "num_posts, vocabulary_size = tf_np_matrix.shape\n",
        "print(f\"Our collection of {num_posts} newsgroup posts contain a total of {vocabulary_size} unique words\")"
      ],
      "metadata": {
        "id": "uCgGVrWjvxJE",
        "outputId": "5ef1f2fc-c8f5-4427-b5ed-fce163543e07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our collection of 11314 newsgroup posts contain a total of 114751 unique words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the unique words in the car post\n",
        "tf_vector = tf_np_matrix[0]\n",
        "non_zero_indices = np.flatnonzero(tf_vector)\n",
        "num_unique_words = non_zero_indices.size\n",
        "\n",
        "print(f\"The newsgroup in row 0 contains {num_unique_words} unique words.\")\n",
        "print(\"The actual word counts map to the following column indices:\\n\")\n",
        "print(non_zero_indices)"
      ],
      "metadata": {
        "id": "1U4kJ20AwNPw",
        "outputId": "20d300c4-f0a7-43c0-d80a-2ddbeb2e6c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The newsgroup in row 0 contains 64 unique words.\n",
            "The actual word counts map to the following column indices:\n",
            "\n",
            "[ 14106  15549  22088  23323  24398  27703  29357  30093  30629  32194\n",
            "  32305  32499  37154  39275  42332  42333  43643  45089  45141  49871\n",
            "  49881  50165  54442  55453  57577  58321  58842  60116  60326  64083\n",
            "  65338  67137  67140  68931  69080  70570  72915  75280  78264  78701\n",
            "  79055  79461  79534  82759  84398  87690  89161  92157  93304  95225\n",
            "  96145  96432 100406 100827 100942 101084 101732 108644 109086 109254\n",
            " 109294 110106 112936 113262]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find a mapping between TF vector indices and word values."
      ],
      "metadata": {
        "id": "js8aNOxOxB9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the unique words in the car post\n",
        "words = vectorizer.get_feature_names()\n",
        "unique_words = [words[i] for i in non_zero_indices]\n",
        "print(unique_words)"
      ],
      "metadata": {
        "id": "ZgDYOtrRxDxS",
        "outputId": "c7d79984-be2b-472f-f114-f621a6f13a53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['60s', '70s', 'addition', 'all', 'anyone', 'be', 'body', 'bricklin', 'bumper', 'called', 'can', 'car', 'could', 'day', 'door', 'doors', 'early', 'engine', 'enlighten', 'from', 'front', 'funky', 'have', 'history', 'if', 'in', 'info', 'is', 'it', 'know', 'late', 'looked', 'looking', 'made', 'mail', 'me', 'model', 'name', 'of', 'on', 'or', 'other', 'out', 'please', 'production', 'really', 'rest', 'saw', 'separate', 'small', 'specs', 'sports', 'tellme', 'the', 'there', 'this', 'to', 'was', 'were', 'whatever', 'where', 'wondering', 'years', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirming first and last word\n",
        "print(words[14106])\n",
        "print(words[113262])"
      ],
      "metadata": {
        "id": "gV7b2lGgxXNd",
        "outputId": "db14a768-6246-45cc-a86d-e6aef55a1976",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60s\n",
            "you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the most frequent words in the car post\n",
        "data = {\"Word\": unique_words, \"Count\": tf_vector[non_zero_indices]}\n",
        "df = pd.DataFrame(data).sort_values(\"Count\", ascending=False)\n",
        "print(df[:10].to_string(index=False))"
      ],
      "metadata": {
        "id": "-CrNSF56ybGK",
        "outputId": "5fb609f1-2c41-4515-b45a-1e0fa5e64f26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Word  Count\n",
            "   the      6\n",
            "  this      4\n",
            "   was      4\n",
            "   car      4\n",
            "    if      2\n",
            "    is      2\n",
            "    it      2\n",
            "  from      2\n",
            "    on      2\n",
            "anyone      2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The common words are a source of noise and increase the likelihood\n",
        "that two unrelated documents will cluster together. \n",
        "\n",
        "NLP practitioners refer to\n",
        "such noisy words as stop words because they are blocked from appearing in the vectorized\n",
        "results. \n",
        "\n",
        "Stop words are generally deleted from the text before vectorization."
      ],
      "metadata": {
        "id": "2vAkj_Ng0qsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stop words during vectorization\n",
        "vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
        "assert tf_matrix.shape[1] < 114751 \n",
        "\n",
        "# Common stop words have been filtered out\n",
        "words = vectorizer.get_feature_names()\n",
        "for common_word in [\"the\", \"this\", \"is\", \"was\", \"if\", \"it\", \"on\"]:\n",
        "  assert common_word not in words"
      ],
      "metadata": {
        "id": "1nu0_ttr0wgu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reprinting the top words after stop-word deletion\n",
        "tf_np_matrix = tf_matrix.toarray()\n",
        "tf_vector = tf_np_matrix[0]\n",
        "non_zero_indices = np.flatnonzero(tf_vector)\n",
        "unique_words = [words[i] for i in non_zero_indices]\n",
        "\n",
        "data = {\"Word\": unique_words, \"Count\": tf_vector[non_zero_indices]}\n",
        "df = pd.DataFrame(data).sort_values(\"Count\", ascending=False)\n",
        "print(f\"After stop-word deletion, {df.shape[0]} unique words remain.\")\n",
        "print(f\"The 10 most frequent words are:\\n\")\n",
        "print(df[:10].to_string(index=False))"
      ],
      "metadata": {
        "id": "mfPeOlNM2FJr",
        "outputId": "1fd1e596-d326-43be-cbe0-6e70561c004d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stop-word deletion, 34 unique words remain.\n",
            "The 10 most frequent words are:\n",
            "\n",
            "      Word  Count\n",
            "       car      4\n",
            "       60s      1\n",
            "       saw      1\n",
            "   looking      1\n",
            "      mail      1\n",
            "     model      1\n",
            "production      1\n",
            "    really      1\n",
            "      rest      1\n",
            "  separate      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ranking words"
      ],
      "metadata": {
        "id": "kYeDlob55BLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ux3DdtdE5B7f"
      }
    }
  ]
}