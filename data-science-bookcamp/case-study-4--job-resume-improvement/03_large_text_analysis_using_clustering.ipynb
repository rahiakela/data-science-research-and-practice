{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-large-text-analysis-using-clustering.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMknYbEXiy5y4G6rpkZ5A0L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/data-science-research-and-practice/blob/main/data-science-bookcamp/case-study-4--job-resume-improvement/03_large_text_analysis_using_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Large text analysis using Clustering"
      ],
      "metadata": {
        "id": "CKQ3xygyzGoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we execute NLP on large collections\n",
        "of real-world texts. This type of analysis is seemingly straightforward, given the\n",
        "techniques presented thus far. For example, suppose we’re doing market research\n",
        "across multiple online discussion forums. Each forum is composed of hundreds of\n",
        "users who discuss a specific topic, such as politics, fashion, technology, or cars. We\n",
        "want to automatically extract all the discussion topics based on the contents of the\n",
        "user conversations. These extracted topics will be used to plan a marketing campaign,\n",
        "which will target users based on their online interests.\n",
        "\n",
        "How do we cluster user discussions into topics? \n",
        "\n",
        "One approach would be to do the following:\n",
        "1. Convert all discussion texts into a matrix of word counts\n",
        "2. Dimensionally reduce the word count matrix using singular value decomposition (SVD). This will allow us to efficiently complete all pairs of text similarities with matrix multiplication.\n",
        "3. Utilize the matrix of text similarities to cluster the discussions into topics.\n",
        "4. Explore the topic clusters to identify useful topics for our marketing campaign.\n",
        "\n"
      ],
      "metadata": {
        "id": "fJsYonsIz1lJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "nEaM8U6Lz_Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "gtDg8Oyyz__L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import time\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pandas as pd\n",
        "import math\n",
        "from math import sin, cos\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import binarize, normalize\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TobbrwVl0CWF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##20Newsgroup dataset"
      ],
      "metadata": {
        "id": "tRYlC_k-0OTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usenet, which is a well-established online collection\n",
        "of discussion forums, are called newsgroups. Each individual\n",
        "newsgroup focuses on some topic of discussion, which is briefly outlined in the newsgroup name.\n",
        "\n",
        "We can load these newsgroup posts by importing `fetch_20newsgroups` from `sklearn.datasets`."
      ],
      "metadata": {
        "id": "sTg51MOn0QLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the newsgroup dataset\n",
        "newsgroups = fetch_20newsgroups(remove=(\"headers\", \"footers\"))"
      ],
      "metadata": {
        "id": "WCepvD5r0Wq4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The newsgroups object contains posts from 20 different newsgroups."
      ],
      "metadata": {
        "id": "uvofbc7T05_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the names of all 20 newsgroups\n",
        "print(newsgroups.target_names)"
      ],
      "metadata": {
        "id": "9gsaUHqR06bp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1417a6cd-45ad-4fe9-abe4-381173420322"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(newsgroups.target_names))"
      ],
      "metadata": {
        "id": "SHYUPbuJ1gpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677c05a4-b7e7-4901-db32-51376a7d3a44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s turn our attention to the actual newsgroup texts, which are stored as a list in the newsgroups.data attribute."
      ],
      "metadata": {
        "id": "w2n8UnF4fk1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the first newsgroup post\n",
        "print(newsgroups.data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANhKc1npf3HO",
        "outputId": "18e274cc-ce28-43d6-ceb8-8c75b2b47287"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the newsgroup name at index 0\n",
        "origin = newsgroups.target_names[newsgroups.target[0]]\n",
        "print(f\"The post at index 0 first appeared in the '{origin}' group.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcYizeuvtt-g",
        "outputId": "7c8e7349-431e-44ac-9c1f-5a1cbe25dcc8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The post at index 0 first appeared in the 'rec.autos' group.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s dive deeper into our newsgroup dataset by printing out the dataset size."
      ],
      "metadata": {
        "id": "aJ3Hq8b4uFwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of newsgroup posts\n",
        "dataset_size = len(newsgroups.data)\n",
        "print(f\"Our dataset contains {dataset_size} newsgroup posts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42LzwACquGMm",
        "outputId": "533b02ad-a436-4c81-a207-1665c0116149"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our dataset contains 11314 newsgroup posts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vectorizing documents"
      ],
      "metadata": {
        "id": "KoPVq13vhrpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need\n",
        "to efficiently compute newsgroup post similarities by representing our text data as a\n",
        "matrix. \n",
        "\n",
        "To do so, we need to transform each newsgroup post into a term-frequency\n",
        "(TF) vector.\n",
        "\n",
        "Scikit-learn provides a `CountVectorizer` class for transforming input texts into TF vectors."
      ],
      "metadata": {
        "id": "i3yrzb_ohs4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing a TF matrix\n",
        "vectorizer = CountVectorizer()\n",
        "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
        "print(tf_matrix)"
      ],
      "metadata": {
        "id": "fcDHOYXThzU9",
        "outputId": "fa051bc8-270f-4d8b-cb10-9ae80c88c20f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 108644)\t4\n",
            "  (0, 110106)\t1\n",
            "  (0, 57577)\t2\n",
            "  (0, 24398)\t2\n",
            "  (0, 79534)\t1\n",
            "  (0, 100942)\t1\n",
            "  (0, 37154)\t1\n",
            "  (0, 45141)\t1\n",
            "  (0, 70570)\t1\n",
            "  (0, 78701)\t2\n",
            "  (0, 101084)\t4\n",
            "  (0, 32499)\t4\n",
            "  (0, 92157)\t1\n",
            "  (0, 100827)\t6\n",
            "  (0, 79461)\t1\n",
            "  (0, 39275)\t1\n",
            "  (0, 60326)\t2\n",
            "  (0, 42332)\t1\n",
            "  (0, 96432)\t1\n",
            "  (0, 67137)\t1\n",
            "  (0, 101732)\t1\n",
            "  (0, 27703)\t1\n",
            "  (0, 49871)\t2\n",
            "  (0, 65338)\t1\n",
            "  (0, 14106)\t1\n",
            "  :\t:\n",
            "  (11313, 55901)\t1\n",
            "  (11313, 93448)\t1\n",
            "  (11313, 97535)\t1\n",
            "  (11313, 93393)\t1\n",
            "  (11313, 109366)\t1\n",
            "  (11313, 102215)\t1\n",
            "  (11313, 29148)\t1\n",
            "  (11313, 26901)\t1\n",
            "  (11313, 94401)\t1\n",
            "  (11313, 89686)\t1\n",
            "  (11313, 80827)\t1\n",
            "  (11313, 72219)\t1\n",
            "  (11313, 32984)\t1\n",
            "  (11313, 82912)\t1\n",
            "  (11313, 99934)\t1\n",
            "  (11313, 96505)\t1\n",
            "  (11313, 72102)\t1\n",
            "  (11313, 32981)\t1\n",
            "  (11313, 82692)\t1\n",
            "  (11313, 101854)\t1\n",
            "  (11313, 66399)\t1\n",
            "  (11313, 63405)\t1\n",
            "  (11313, 61366)\t1\n",
            "  (11313, 7462)\t1\n",
            "  (11313, 109600)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the data type\n",
        "print(type(tf_matrix))"
      ],
      "metadata": {
        "id": "KlMzEhydvUtn",
        "outputId": "18566b99-3a62-49ea-9bb8-36659e590602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting a CSR matrix to a NumPy array\n",
        "tf_np_matrix = tf_matrix.toarray()\n",
        "print(tf_np_matrix)"
      ],
      "metadata": {
        "id": "OQ3Az3Wpvci9",
        "outputId": "0b037c92-f284-45f2-ca70-5e8b2699702b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the vocabulary size\n",
        "assert tf_np_matrix.shape == tf_matrix.shape\n",
        "num_posts, vocabulary_size = tf_np_matrix.shape\n",
        "print(f\"Our collection of {num_posts} newsgroup posts contain a total of {vocabulary_size} unique words\")"
      ],
      "metadata": {
        "id": "uCgGVrWjvxJE",
        "outputId": "bb561d41-0b29-40d8-d8f7-0924ef9827c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our collection of 11314 newsgroup posts contain a total of 114751 unique words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the unique words in the car post\n",
        "tf_vector = tf_np_matrix[0]\n",
        "non_zero_indices = np.flatnonzero(tf_vector)\n",
        "num_unique_words = non_zero_indices.size\n",
        "\n",
        "print(f\"The newsgroup in row 0 contains {num_unique_words} unique words.\")\n",
        "print(\"The actual word counts map to the following column indices:\\n\")\n",
        "print(non_zero_indices)"
      ],
      "metadata": {
        "id": "1U4kJ20AwNPw",
        "outputId": "e9266cac-1af4-4ec1-e9ce-3a409312f0cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The newsgroup in row 0 contains 64 unique words.\n",
            "The actual word counts map to the following column indices:\n",
            "\n",
            "[ 14106  15549  22088  23323  24398  27703  29357  30093  30629  32194\n",
            "  32305  32499  37154  39275  42332  42333  43643  45089  45141  49871\n",
            "  49881  50165  54442  55453  57577  58321  58842  60116  60326  64083\n",
            "  65338  67137  67140  68931  69080  70570  72915  75280  78264  78701\n",
            "  79055  79461  79534  82759  84398  87690  89161  92157  93304  95225\n",
            "  96145  96432 100406 100827 100942 101084 101732 108644 109086 109254\n",
            " 109294 110106 112936 113262]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find a mapping between TF vector indices and word values."
      ],
      "metadata": {
        "id": "js8aNOxOxB9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the unique words in the car post\n",
        "words = vectorizer.get_feature_names()\n",
        "unique_words = [words[i] for i in non_zero_indices]\n",
        "print(unique_words)"
      ],
      "metadata": {
        "id": "ZgDYOtrRxDxS",
        "outputId": "f4a80fab-ada2-49ab-aac4-2314de21eaaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['60s', '70s', 'addition', 'all', 'anyone', 'be', 'body', 'bricklin', 'bumper', 'called', 'can', 'car', 'could', 'day', 'door', 'doors', 'early', 'engine', 'enlighten', 'from', 'front', 'funky', 'have', 'history', 'if', 'in', 'info', 'is', 'it', 'know', 'late', 'looked', 'looking', 'made', 'mail', 'me', 'model', 'name', 'of', 'on', 'or', 'other', 'out', 'please', 'production', 'really', 'rest', 'saw', 'separate', 'small', 'specs', 'sports', 'tellme', 'the', 'there', 'this', 'to', 'was', 'were', 'whatever', 'where', 'wondering', 'years', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirming first and last word\n",
        "print(words[14106])\n",
        "print(words[113262])"
      ],
      "metadata": {
        "id": "gV7b2lGgxXNd",
        "outputId": "b314dba1-c40a-4e85-e818-5c88dff73100",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60s\n",
            "you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the most frequent words in the car post\n",
        "data = {\"Word\": unique_words, \"Count\": tf_vector[non_zero_indices]}\n",
        "df = pd.DataFrame(data).sort_values(\"Count\", ascending=False)\n",
        "print(df[:10].to_string(index=False))"
      ],
      "metadata": {
        "id": "-CrNSF56ybGK",
        "outputId": "76602ff1-434d-4fb7-85eb-b222a1e90bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Word  Count\n",
            "   the      6\n",
            "  this      4\n",
            "   was      4\n",
            "   car      4\n",
            "    if      2\n",
            "    is      2\n",
            "    it      2\n",
            "  from      2\n",
            "    on      2\n",
            "anyone      2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# free memory\n",
        "del tf_matrix\n",
        "del tf_np_matrix\n",
        "del tf_vector"
      ],
      "metadata": {
        "id": "Wk1n2eI_5H9T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The common words are a source of noise and increase the likelihood\n",
        "that two unrelated documents will cluster together. \n",
        "\n",
        "NLP practitioners refer to\n",
        "such noisy words as stop words because they are blocked from appearing in the vectorized\n",
        "results. \n",
        "\n",
        "Stop words are generally deleted from the text before vectorization."
      ],
      "metadata": {
        "id": "2vAkj_Ng0qsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stop words during vectorization\n",
        "vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
        "assert tf_matrix.shape[1] < 114751 \n",
        "\n",
        "# Common stop words have been filtered out\n",
        "words = vectorizer.get_feature_names()\n",
        "for common_word in [\"the\", \"this\", \"is\", \"was\", \"if\", \"it\", \"on\"]:\n",
        "  assert common_word not in words"
      ],
      "metadata": {
        "id": "1nu0_ttr0wgu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reprinting the top words after stop-word deletion\n",
        "tf_np_matrix = tf_matrix.toarray()\n",
        "tf_vector = tf_np_matrix[0]\n",
        "non_zero_indices = np.flatnonzero(tf_vector)\n",
        "unique_words = [words[i] for i in non_zero_indices]\n",
        "\n",
        "data = {\"Word\": unique_words, \"Count\": tf_vector[non_zero_indices]}\n",
        "df = pd.DataFrame(data).sort_values(\"Count\", ascending=False)\n",
        "print(f\"After stop-word deletion, {df.shape[0]} unique words remain.\")\n",
        "print(f\"The 10 most frequent words are:\\n\")\n",
        "print(df[:10].to_string(index=False))"
      ],
      "metadata": {
        "id": "mfPeOlNM2FJr",
        "outputId": "53627f5a-dd24-4e45-d9f1-2948ac2dbb84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stop-word deletion, 34 unique words remain.\n",
            "The 10 most frequent words are:\n",
            "\n",
            "      Word  Count\n",
            "       car      4\n",
            "       60s      1\n",
            "       saw      1\n",
            "   looking      1\n",
            "      mail      1\n",
            "     model      1\n",
            "production      1\n",
            "    really      1\n",
            "      rest      1\n",
            "  separate      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ranking words"
      ],
      "metadata": {
        "id": "kYeDlob55BLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of the 34 words in df.Word appears in a certain fraction of newsgroup posts. In\n",
        "NLP, this fraction is referred to as the document frequency of a word. \n",
        "\n",
        "We hypothesize\n",
        "that the document frequencies can improve our word rankings.\n",
        "\n",
        "We can compute these frequencies\n",
        "using a series of NumPy matrix manipulations."
      ],
      "metadata": {
        "id": "Ux3DdtdE5B7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering matrix columns with non_zero_indices\n",
        "sub_matrix = tf_np_matrix[:, non_zero_indices]\n",
        "print(\"Our sub-matrix corresponds to the 34 words within post 0.\\nThe first row of the sub-matrix is:\")\n",
        "print(sub_matrix[0])"
      ],
      "metadata": {
        "id": "WyJBvXn_RlJl",
        "outputId": "eac70846-e5fe-4099-bba7-5b26341d6e38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our sub-matrix corresponds to the 34 words within post 0.\n",
            "The first row of the sub-matrix is:\n",
            "[1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we are not currently interested\n",
        "in exact word counts: we just want to know whether each word is present or\n",
        "absent from each post. \n",
        "\n",
        "So, we need to convert our counts into binary values."
      ],
      "metadata": {
        "id": "tu-Sk9szSD7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting word counts to binary values\n",
        "binary_matrix = binarize(sub_matrix)\n",
        "print(binary_matrix)"
      ],
      "metadata": {
        "id": "i4NASNA1SI8q",
        "outputId": "cc69bbc8-e69b-40f9-fd6d-0b9f47df6350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 ... 1 1 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to add together the rows of our binary submatrix. \n",
        "\n",
        "Doing so will produce\n",
        "a vector of integer counts."
      ],
      "metadata": {
        "id": "ZaEd9JthShft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summing matrix rows to obtain post counts\n",
        "unique_post_mentions = binary_matrix.sum(axis=0)\n",
        "print(f\"This vector counts the unique posts in which each word is mentioned:\\n {unique_post_mentions}\")"
      ],
      "metadata": {
        "id": "NZHejAdwSiH_",
        "outputId": "d584699d-5398-4428-a11f-5c11dc717ef7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This vector counts the unique posts in which each word is mentioned:\n",
            " [  18   21  202  314    4   26  802  536  842  154   67  348  184   25\n",
            "    7  368  469 3093  238  268  780  901  292   95 1493  407  354  158\n",
            "  574   95   98    2  295 1174]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing post mention counts in a single line of code\n",
        "np_post_mentions = binarize(tf_np_matrix[:, non_zero_indices]).sum(axis=0)\n",
        "csr_post_mentions = binarize(tf_matrix[:, non_zero_indices]).sum(axis=0)\n",
        "\n",
        "print(f\"f'NumPy matrix-generated counts:\\n {np_post_mentions}\\n\")\n",
        "print(f\"CSR matrix-generated counts:\\n {csr_post_mentions}\")"
      ],
      "metadata": {
        "id": "HCGhAF4HTBq8",
        "outputId": "5402a209-4cf8-41e5-fd6f-c51ae9c0fb7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f'NumPy matrix-generated counts:\n",
            " [  18   21  202  314    4   26  802  536  842  154   67  348  184   25\n",
            "    7  368  469 3093  238  268  780  901  292   95 1493  407  354  158\n",
            "  574   95   98    2  295 1174]\n",
            "\n",
            "CSR matrix-generated counts:\n",
            " [[  18   21  202  314    4   26  802  536  842  154   67  348  184   25\n",
            "     7  368  469 3093  238  268  780  901  292   95 1493  407  354  158\n",
            "   574   95   98    2  295 1174]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numbers in `np_post_mentions` and `csr_post_mentions` appear identical.\n",
        "\n",
        "Let’s transform\n",
        "these counts into document frequencies and align the frequencies with `df.Word`."
      ],
      "metadata": {
        "id": "HNxDW7AKTlMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the words with the highest document frequency\n",
        "document_frequencies = unique_post_mentions / dataset_size\n",
        "data = {\"Word\": unique_words, \"Count\": tf_vector[non_zero_indices], \"DF\": document_frequencies}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "# choose words with a document frequency greater than 1/10\n",
        "df_common_words = df[df[\"DF\"] >= .1]\n",
        "print(df_common_words.to_string(index=False))"
      ],
      "metadata": {
        "id": "dFLRlJ4gaZFr",
        "outputId": "152c439f-f71f-4fdb-919e-d84b20269666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Word  Count       DF\n",
            "  know      1 0.273378\n",
            "really      1 0.131960\n",
            " years      1 0.103765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, these words are very general and not car specific. We thus can utilize document frequencies for ranking purposes.\n",
        "\n",
        "Let’s rank our words by relevance in the following\n",
        "manner. First, we sort the words by count, from greatest to smallest. \n",
        "\n",
        "Then, all words\n",
        "with equal count are sorted by document frequency, from smallest to greatest."
      ],
      "metadata": {
        "id": "87xdJdCecXXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ranking words by both count and document frequency\n",
        "df_sorted = df.sort_values([\"Count\", \"DF\"], ascending=[False, True])\n",
        "print(df_sorted[:10].to_string(index=False))"
      ],
      "metadata": {
        "id": "EwKgADAwchpA",
        "outputId": "22ec4471-f77f-4e2a-8612-baf07b928a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Word  Count       DF\n",
            "       car      4 0.047375\n",
            "    tellme      1 0.000177\n",
            "  bricklin      1 0.000354\n",
            "     funky      1 0.000619\n",
            "       60s      1 0.001591\n",
            "       70s      1 0.001856\n",
            " enlighten      1 0.002210\n",
            "    bumper      1 0.002298\n",
            "     doors      1 0.005922\n",
            "production      1 0.008397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our sorting was successful. New car-related words, such as bumper, are now present in\n",
        "our list of top-ranked words.\n",
        "\n",
        "However, the actual sorting procedure was rather convoluted:\n",
        "it required us to sort two columns separately. \n",
        "\n",
        "Perhaps we can simplify the process\n",
        "by combining the word counts and document frequencies into a single score.\n",
        "\n",
        "How can we do this? \n",
        "\n",
        "One approach is to divide each word count by its associated document\n",
        "frequency. \n",
        "\n",
        "The resulting value will increase if either of the following is true:\n",
        "* The word count goes up.\n",
        "* The document frequency goes down.\n",
        "\n",
        "Let’s combine the word counts and the document frequencies into a single score. We\n",
        "start by computing 1 / document_frequencies. \n",
        "\n",
        "Doing so produces an array of inverse\n",
        "document frequencies (IDFs). \n",
        "\n",
        "Next, we multiply df.Count by the IDF array to compute\n",
        "the combined score."
      ],
      "metadata": {
        "id": "YRznjEBmc80v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining counts and frequencies into a single score\n",
        "inverse_document_frequencies = 1 / document_frequencies\n",
        "df[\"IDF\"] = inverse_document_frequencies\n",
        "df[\"Combined\"] = df.Count * inverse_document_frequencies\n",
        "df_sorted = df.sort_values(\"Combined\", ascending=False)\n",
        "print(df_sorted[:10].to_string(index=False))"
      ],
      "metadata": {
        "id": "FTccxQJBdguz",
        "outputId": "dcb3461b-886e-4ada-8974-08ee3c1da945",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Word  Count       DF         IDF    Combined\n",
            "    tellme      1 0.000177 5657.000000 5657.000000\n",
            "  bricklin      1 0.000354 2828.500000 2828.500000\n",
            "     funky      1 0.000619 1616.285714 1616.285714\n",
            "       60s      1 0.001591  628.555556  628.555556\n",
            "       70s      1 0.001856  538.761905  538.761905\n",
            " enlighten      1 0.002210  452.560000  452.560000\n",
            "    bumper      1 0.002298  435.153846  435.153846\n",
            "     doors      1 0.005922  168.865672  168.865672\n",
            "     specs      1 0.008397  119.094737  119.094737\n",
            "production      1 0.008397  119.094737  119.094737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our new ranking failed! The word car no longer appears at the top of the list.\n",
        "\n",
        "There is a problem with the IDF values: some of them are huge!\n",
        "\n",
        "Meanwhile, our word-count range is very small: from 1 to 4. \n",
        "\n",
        "Thus, when we multiply\n",
        "word counts by IDF values, the IDF dominates, and the counts have no impact on the final results. We need to somehow make our IDF values smaller. \n",
        "\n",
        "What should we do?\n",
        "\n",
        "Data scientists are commonly confronted with numeric values that are too large.\n",
        "\n",
        "**One way to shrink the values is to apply a logarithmic function.**"
      ],
      "metadata": {
        "id": "brf2eMKbeFtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shrinking a large value using its logarithm\n",
        "assert np.log10(1000000) == 6\n",
        "assert np.log10(10000) == 4\n",
        "assert np.log10(100) == 2\n",
        "assert np.log10(10) == 1\n",
        "assert np.log10(1) == 0\n",
        "assert np.log10(0) == -np.inf\n",
        "assert math.isnan(np.log10(-1)) == math.isnan(float('nan'))"
      ],
      "metadata": {
        "id": "1JJODyndemIU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s recompute our ranking score."
      ],
      "metadata": {
        "id": "r0apZn0RiH-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusting the combined score using logarithms\n",
        "df[\"Combined\"] = df.Count * np.log10(df.IDF)\n",
        "df_sorted = df.sort_values(\"Combined\", ascending=False)\n",
        "print(df_sorted[:10].to_string(index=False))"
      ],
      "metadata": {
        "id": "hHAs0djiiImy",
        "outputId": "7f58cb3b-303c-4406-b423-ab86914240a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Word  Count       DF         IDF  Combined\n",
            "      car      4 0.047375   21.108209  5.297806\n",
            "   tellme      1 0.000177 5657.000000  3.752586\n",
            " bricklin      1 0.000354 2828.500000  3.451556\n",
            "    funky      1 0.000619 1616.285714  3.208518\n",
            "      60s      1 0.001591  628.555556  2.798344\n",
            "      70s      1 0.001856  538.761905  2.731397\n",
            "enlighten      1 0.002210  452.560000  2.655676\n",
            "   bumper      1 0.002298  435.153846  2.638643\n",
            "    doors      1 0.005922  168.865672  2.227541\n",
            "    specs      1 0.008397  119.094737  2.075893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our adjusted ranking score has yielded good results. The word car is once again present\n",
        "at the top of the ranked list. \n",
        "\n",
        "Also, bumper still appears among the top 10 ranked\n",
        "words. Meanwhile, really is missing from the list.\n",
        "\n",
        "**Our effective score is called the term frequency-inverse document frequency (TFIDF)**.\n",
        "\n",
        "The TFIDF can be computed by taking the product of the TF (word count) and the\n",
        "log of the IDF.\n",
        "\n",
        "Mathematically, $np.log(1 / x)$ is equal to $-np.log(x)$. \n",
        "\n",
        "Therefore, we\n",
        "can compute the TFIDF directly from the document frequencies."
      ],
      "metadata": {
        "id": "CS6hkh-UigQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Combined\"] = df.Count * -np.log10(document_frequencies)\n",
        "df_sorted = df.sort_values(\"Combined\", ascending=False)\n",
        "print(df_sorted[:10].to_string(index=False))"
      ],
      "metadata": {
        "id": "Wiu5624Wi8WK",
        "outputId": "3760254e-48f7-4bb1-fe43-c608e38b1617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Word  Count       DF         IDF  Combined\n",
            "      car      4 0.047375   21.108209  5.297806\n",
            "   tellme      1 0.000177 5657.000000  3.752586\n",
            " bricklin      1 0.000354 2828.500000  3.451556\n",
            "    funky      1 0.000619 1616.285714  3.208518\n",
            "      60s      1 0.001591  628.555556  2.798344\n",
            "      70s      1 0.001856  538.761905  2.731397\n",
            "enlighten      1 0.002210  452.560000  2.655676\n",
            "   bumper      1 0.002298  435.153846  2.638643\n",
            "    doors      1 0.005922  168.865672  2.227541\n",
            "    specs      1 0.008397  119.094737  2.075893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TFIDF is a simple but powerful metric for ranking words in a document. Of\n",
        "course, the metric is only relevant if that document is part of a larger document group. \n",
        "\n",
        "Otherwise, the computed TFIDF values all equal zero.\n",
        "\n",
        "And it has\n",
        "additional uses: it can be utilized to vectorize words in a document.\n",
        "\n",
        "In this same manner, we can transform any TF vector into a\n",
        "TFIDF vector. We just need to multiply the TF vector by the log of inverse document frequencies.\n",
        "\n",
        "Is there a benefit to transforming TF vectors into more complicated TFIDF vectors?\n",
        "\n",
        "Yes! In larger text datasets, TFIDF vectors provide a greater signal of textual similarity\n",
        "and divergence.\n",
        "\n",
        "For example, two texts that are both discussing cars are more\n",
        "likely to cluster together if their irrelevant vector elements are penalized.\n",
        "\n",
        "Thus, **penalizing common words using the IDF improves the clustering of large text collections.**\n",
        "\n",
        "We therefore stand to gain by transforming our TF matrix into a TFIDF matrix."
      ],
      "metadata": {
        "id": "DLTgLVywjP1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# free memory\n",
        "del tf_matrix\n",
        "del tf_np_matrix\n",
        "del tf_vector"
      ],
      "metadata": {
        "id": "Z5vI3h5e1Dtk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Computing TF-IDF vectors"
      ],
      "metadata": {
        "id": "3dmq03LBiwvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That `TfidfVectorizer` class is nearly identical to `CountVectorizer`, except that it\n",
        "takes IDF into account during the vectorization process."
      ],
      "metadata": {
        "id": "bBuP56oJix_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing a TFIDF matrix with scikit-learn\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(newsgroups.data)\n",
        "\n",
        "#assert tfidf_matrix.shape == tf_matrix.shape"
      ],
      "metadata": {
        "id": "s05FPhwGtCUk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirming the preservation of vectorized word indices\n",
        "assert tfidf_vectorizer.get_feature_names() == words"
      ],
      "metadata": {
        "id": "Fs39n6AytoG-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since word order is preserved, we should expect the nonzero indices of `tfidf_\n",
        "matrix[0]` to equal our previously computed `non_zero_indices` array."
      ],
      "metadata": {
        "id": "jJXkmX25tymd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirming the preservation of nonzero indices\n",
        "tfidf_np_matrix = tfidf_matrix.toarray()\n",
        "tfidf_vector = tfidf_np_matrix[0]\n",
        "tfidf_non_zero_indices = np.flatnonzero(tfidf_vector)\n",
        "\n",
        "# The nonzero indices of tf_vector and tfidf_vector are identical.\n",
        "assert np.array_equal(tfidf_non_zero_indices, non_zero_indices)"
      ],
      "metadata": {
        "id": "aHY88trqt2GS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a TFIDF vector to the existing Pandas table\n",
        "df[\"TF-IDF\"] = tfidf_vector[non_zero_indices]"
      ],
      "metadata": {
        "id": "wXAqWsYP1xEN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting by `df.TF-IDF` should produce a relevance ranking that is consistent with our\n",
        "previous observations. \n",
        "\n",
        "Let’s verify that both `df.TF-IDF` and `df.Combined` produce the\n",
        "same word rankings after sorting."
      ],
      "metadata": {
        "id": "9ndK8fV4zREe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting words by df.TFIDF\n",
        "df_sorted_old = df.sort_values(\"Combined\", ascending=False)\n",
        "df_sorted_new = df.sort_values(\"TF-IDF\", ascending=False)\n",
        "\n",
        "assert np.array_equal(df_sorted_old[\"Word\"].values, df_sorted_new[\"Word\"].values)\n",
        "\n",
        "print(df_sorted_new[:10].to_string(index=False))"
      ],
      "metadata": {
        "id": "ARl9Hh_zzWZ-",
        "outputId": "b0170ec7-4e87-409b-ea7b-251d530c91f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Word  Count       DF         IDF  Combined   TF-IDF\n",
            "      car      4 0.047375   21.108209  5.297806 0.459552\n",
            "   tellme      1 0.000177 5657.000000  3.752586 0.262118\n",
            " bricklin      1 0.000354 2828.500000  3.451556 0.247619\n",
            "    funky      1 0.000619 1616.285714  3.208518 0.234280\n",
            "      60s      1 0.001591  628.555556  2.798344 0.209729\n",
            "      70s      1 0.001856  538.761905  2.731397 0.205568\n",
            "enlighten      1 0.002210  452.560000  2.655676 0.200827\n",
            "   bumper      1 0.002298  435.153846  2.638643 0.199756\n",
            "    doors      1 0.005922  168.865672  2.227541 0.173540\n",
            "    specs      1 0.008397  119.094737  2.075893 0.163752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our word rankings have remained unchanged. However, the values of the `TF-IDF` and `Combined` columns are not identical.\n",
        "\n",
        "Why is this the case?\n",
        "\n",
        "As it turns out, scikit-learn automatically normalizes its `TFIDF` vector results. \n",
        "\n",
        "The magnitude of `df.TF-IDF` has been modified to equal 1. \n",
        "\n",
        "We can confirm by calling\n",
        "`norm(df.TFIDF.values)`."
      ],
      "metadata": {
        "id": "SVU-mE2h5cwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirming that our TFIDF vector is normalized\n",
        "assert norm(df[\"TF-IDF\"].values) == 1"
      ],
      "metadata": {
        "id": "su5XXaUg5xab"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why would scikit-learn automatically normalize the vectors?\n",
        "\n",
        "As discussed, it’s easier to compute text vector similarity when all vector\n",
        "magnitudes equal 1. \n",
        "\n",
        "Consequently, our normalized TFIDF matrix is primed for similarity analysis.\n",
        "\n",
        ">To turn off normalization, we must pass `norm=None` into the vectorizer’s\n",
        "initialization function. Running `TfidfVectorizer(norm=None, stop_\n",
        "words='english')` returns a vectorizer in which normalization has been\n",
        "deactivated."
      ],
      "metadata": {
        "id": "lAdpfk2K6Qdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Computing document similarities"
      ],
      "metadata": {
        "id": "JHKYcCEG6rS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s answer a simple question: \n",
        "\n",
        "which of our newsgroup posts is most similar to `newsgroups.post[0]`?\n",
        "\n",
        "We can get the answer by computing all the cosine similarities\n",
        "between `tfidf_np_matrix` and `tfidf_np_matrix[0]`.\n",
        "\n",
        "The simple multiplication between the matrix and the vector is sufficient\n",
        "because all rows in the matrix have a magnitude of 1."
      ],
      "metadata": {
        "id": "KZiumWfm6ufh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing similarities to a single newsgroup post\n",
        "cosine_similarities = tfidf_np_matrix @ tfidf_np_matrix[0]\n",
        "print(cosine_similarities)"
      ],
      "metadata": {
        "id": "LT5koD209OIx",
        "outputId": "35a84833-3efb-4eae-d344-cbc0846f1c1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.         0.00834093 0.04448717 ... 0.         0.00270615 0.01968562]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the printout, we can see that `cosine_similarities[0]` is equal to 1.0. \n",
        "\n",
        "This is not surprising since `newsgroups_data[0]` will have a perfect similarity with itself.\n",
        "\n",
        "What is the next-highest similarity in the vector?\n",
        "\n",
        "The argsort call sorts the array indices by their ascending values. \n",
        "\n",
        "So, the second-tolast\n",
        "index will correspond to the post with the second-highest similarity."
      ],
      "metadata": {
        "id": "vj85lSRb9vmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the most similar newsgroup post\n",
        "most_similar_index = np.argsort(cosine_similarities)[-2]\n",
        "similarity = cosine_similarities[most_similar_index]\n",
        "most_similar_post = newsgroups.data[most_similar_index]\n",
        "\n",
        "print(f\"The following post has a cosine similarity of {similarity:.2f} with newsgroups.data[0]:\\n\")\n",
        "print(\"-\"*100)\n",
        "print(most_similar_post)\n",
        "print(\"-\"*100)\n",
        "print(newsgroups.data[0])"
      ],
      "metadata": {
        "id": "sv9coE-t-m4P",
        "outputId": "5d382081-abf2-4731-c1df-d8bf670533fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following post has a cosine similarity of 0.64 with newsgroups.data[0]:\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "In article <1993Apr20.174246.14375@wam.umd.edu> lerxst@wam.umd.edu (where's my  \n",
            "thing) writes:\n",
            "> \n",
            ">  I was wondering if anyone out there could enlighten me on this car I saw\n",
            "> the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "> early 70s. It was called a Bricklin. The doors were really small. In  \n",
            "addition,\n",
            "> the front bumper was separate from the rest of the body. This is \n",
            "> all I know. If anyone can tellme a model name, engine specs, years\n",
            "> of production, where this car is made, history, or whatever info you\n",
            "> have on this funky looking car, please e-mail.\n",
            "\n",
            "Bricklins were manufactured in the 70s with engines from Ford. They are rather  \n",
            "odd looking with the encased front bumper. There aren't a lot of them around,  \n",
            "but Hemmings (Motor News) ususally has ten or so listed. Basically, they are a  \n",
            "performance Ford with new styling slapped on top.\n",
            "\n",
            ">    ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "Rush fan?\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The printed text is a reply to the car post at index 0.\n",
        "\n",
        "Due to textual overlap, both the original\n",
        "post and the reply are very similar to each other. Their cosine similarity is 0.64, which does not seem like a large number.\n",
        "\n",
        "Let’s extend our analysis to another post. We’ll pick a newsgroup post at random, choose its most similar neighbor, and then output both posts, along with their cosine similarity. \n",
        "\n",
        "To make this exercise\n",
        "more interesting, we’ll first compute a matrix of all-by-all cosine similarities. We’ll\n",
        "then use the matrix to select our random pair of similar posts.\n",
        "\n",
        "How do we compute the matrix of all-by-all cosine similarities?\n",
        "\n",
        "The naive approach is to multiply tfidf_np_matrix with its transpose. \n",
        "\n",
        "However, this matrix multiplication is not computationally efficient. Our TFIDF matrix\n",
        "has over 100,000 columns. We need to reduce the matrix size before executing the multiplication."
      ],
      "metadata": {
        "id": "vHCpBzRL_zqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionally reducing tfidf_matrix using SVD\n",
        "np.random.seed(0)\n",
        "\n",
        "shrunk_matrix = TruncatedSVD(n_components=100).fit_transform(tfidf_matrix)\n",
        "print(f\"We've dimensionally reduced a {tfidf_matrix.shape[1]}-column {type(tfidf_matrix)} matrix\")\n",
        "print(f\"Our output is a {shrunk_matrix.shape[1]}-column {type(shrunk_matrix)} matrix\")"
      ],
      "metadata": {
        "id": "1QzCPw6cAmLj",
        "outputId": "5f887605-1a54-40a4-e2ee-2817757f8b54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We've dimensionally reduced a 114441-column <class 'scipy.sparse.csr.csr_matrix'> matrix\n",
            "Our output is a 100-column <class 'numpy.ndarray'> matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now efficiently compute the\n",
        "cosine similarities by running `shrunk_matrix @ shrunk_matrix.T`. \n",
        "\n",
        "However, first we\n",
        "need to confirm that the matrix rows remain normalized."
      ],
      "metadata": {
        "id": "QUQjuFeZBcwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the magnitude of shrunk_matrix[0]\n",
        "magnitude = norm(shrunk_matrix[0])\n",
        "print(f\"The magnitude of the first row is {magnitude:.2f}\")"
      ],
      "metadata": {
        "id": "3vh-Ffx_BfR1",
        "outputId": "5c91450f-95d5-4b51-c6a9-1c1e40cf4c27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The magnitude of the first row is 0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The magnitude of the row is less than 1.\n",
        "\n",
        "We need to manually normalize the matrix before computing\n",
        "the similarities."
      ],
      "metadata": {
        "id": "cxYziCMhB6pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the SVD output\n",
        "shrunk_norm_matrix = normalize(shrunk_matrix)\n",
        "magnitude = norm(shrunk_norm_matrix[0])\n",
        "print(f\"The magnitude of the first row is {magnitude:.2f}\")"
      ],
      "metadata": {
        "id": "2IXOeJntB9Mf",
        "outputId": "803c147b-31bf-4aae-c202-96e71e0ec756",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The magnitude of the first row is 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# free memory\n",
        "del shrunk_matrix\n",
        "\n",
        "# Computing all-by-all cosine similarities\n",
        "cosine_similarity_matrix = shrunk_norm_matrix @ shrunk_norm_matrix.T"
      ],
      "metadata": {
        "id": "XlnOFfTvCbiZ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# free memory\n",
        "del shrunk_norm_matrix"
      ],
      "metadata": {
        "id": "Axxz2924FAig"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s use it to choose a random pair of very similar\n",
        "texts."
      ],
      "metadata": {
        "id": "5UvBuANxDK77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing a random pair of similar posts\n",
        "np.random.seed(1)\n",
        "\n",
        "index1 = np.random.randint(dataset_size)\n",
        "index2 = np.argsort(cosine_similarity_matrix[index1])[-2]\n",
        "similarity = cosine_similarity_matrix[index1][index2]\n",
        "\n",
        "print(f\"The posts at indices {index1} and {index2} share a cosine similarity of {similarity:.2f}\")"
      ],
      "metadata": {
        "id": "xvqjzQ1bDLjn",
        "outputId": "6794ce1b-628d-49b7-f5e5-d64ef4cc9c56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The posts at indices 235 and 7805 share a cosine similarity of 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing a randomly chosen post\n",
        "print(newsgroups.data[index2].replace(\"\\n\\n\", \"\\n\"))"
      ],
      "metadata": {
        "id": "uHQ1WaCiFRZR",
        "outputId": "345a2080-8efb-4bed-b438-54b173d04aa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello,\n",
            "\tWho can tell me   Where can I find the PD or ShareWare   \n",
            "Which can CAPTURE windows 3.1's output of printer mananger?\n",
            "\tI want to capture the output of HP Laser Jet III.\n",
            "\tThough the PostScript can setup to print to file,but HP can't.\n",
            "\tI try DOS's redirect program,but they can't work in Windows 3.1\n",
            "\t\tThankx for any help....\n",
            "--\n",
            " Internet Address: u7911093@cc.nctu.edu.tw\n",
            "    English Name: Erik Wang\n",
            "    Chinese Name: Wang Jyh-Shyang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, the printed post is a question. \n",
        "\n",
        "It’s safe to assume that the post at `index1` is\n",
        "an answer to that question."
      ],
      "metadata": {
        "id": "J7Mb115HFkv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the most similar post response\n",
        "print(newsgroups.data[index1].replace(\"\\n\\n\", \"\\n\"))"
      ],
      "metadata": {
        "id": "0sGkRg4KFmEJ",
        "outputId": "7954fdd1-9f4d-4854-f595-3b0d1befd37d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "u7911093@cc.nctu.edu.tw (\"By SWH ) writes:\n",
            ">Who can tell me which program (PD or ShareWare) can redirect windows 3.1's\n",
            ">output of printer manager to file? \n",
            ">\tI want to capture HP Laser Jet III's print output.\n",
            "> \tThough PostScript can setup print to file,but HP can't.\n",
            ">\tI use DOS's redirect program,but they can't work in windows.\n",
            ">\t\tThankx for any help...\n",
            ">--\n",
            "> Internet Address: u7911093@cc.nctu.edu.tw\n",
            ">    English Name: Erik Wang\n",
            ">    Chinese Name: Wang Jyh-Shyang\n",
            "> National Chiao-Tung University,Taiwan,R.O.C.\n",
            "Try setting up another HPIII printer but when choosing what port to connect it\n",
            "to choose FILE instead of like :LPT1.  This will prompt you for a file name\n",
            "everytime you print with that \"HPIII on FILE\" printer. Good Luck.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus far, we have examined two pairs of similar posts. Each post pair was composed of\n",
        "a question and a reply, where the question was included in the reply. Such boring\n",
        "pairs of overlapping texts are trivial to extract."
      ],
      "metadata": {
        "id": "SH_FXKdoGBYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clustering texts by topic"
      ],
      "metadata": {
        "id": "E3OxTQdGGDAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s challenge ourselves to find something\n",
        "more interesting. \n",
        "\n",
        "We’ll search for clusters of similar texts where posts in a cluster\n",
        "share some text without perfectly overlapping."
      ],
      "metadata": {
        "id": "0SSw12ytGFXD"
      }
    }
  ]
}